{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "    - AdaBoost é um algoritmo que cria stump trees, enquanto o GBM cria árvores completas.\n",
    "    - AdaBoost têm votação ponderada, enquanto o GBM têm votação uniforme. Um multiplica o peso de cada stump tree pela sua precisão, enquanto o outro soma os resíduos dos modelos.\n",
    "    - AdaBoost ajusta os pesos dos stumps trees, enquanto o GBM ajusta os resíduos dos modelos com um learning rate que gradualmente reduz o erro.\n",
    "    - AdaBoost faz reamostragem dos dados para corrigir erros enquando o GBM já se ajusta aos resíduos das respostas do modelo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de uso do HistGradientBoostingClassifier direto do sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = HistGradientBoostingClassifier().fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299589575098558"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de uso do HistGradientBoostingRegressor direto do sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "est = HistGradientBoostingRegressor().fit(X, y)\n",
    "est.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "    Meu top 5:\n",
    "\n",
    "        - learning_rate: Taxa de aprendizado do modelo.\n",
    "        - loss: Função de perda do modelo.\n",
    "        - l2_regularization: Regularização L2 do modelo.\n",
    "        - max_depth: Profundidade máxima das árvores.\n",
    "        - max_iter: Número máximo de iterações do modelo.\n",
    "\n",
    "    Todos os hyperparametros do GBM:*\n",
    "\n",
    "        - loss: Literal['log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'] = \"log_loss\",    *,\n",
    "        - learning_rate: Float = 0.1,\n",
    "        - max_iter: Int = 100,\n",
    "        - max_leaf_nodes: int | None = 31,\n",
    "        - max_depth: int | None = None,\n",
    "        - min_samples_leaf: Int = 20,\n",
    "        - l2_regularization: Float = 0,\n",
    "        - max_bins: Int = 255,\n",
    "        - categorical_features: MatrixLike | ArrayLike | None = None,\n",
    "        - monotonic_cst: ArrayLike | Mapping | None = None,\n",
    "        - interaction_cst: Sequence[list[int] | tuple[int, ...] | set[int]] | Literal['pairwise', 'no_interaction'] | None = None,\n",
    "        - warm_start: bool = False,\n",
    "        - early_stopping: bool | Literal['auto'] = \"auto\",\n",
    "        - scoring: str | ((...) -> Any) | None = \"loss\",\n",
    "        - validation_fraction: float | int | None = 0.1,\n",
    "        - n_iter_no_change: Int = 10,\n",
    "        - tol: Float = 1e-7,\n",
    "        - verbose: Int = 0,\n",
    "        - random_state: Int | RandomState | None = None,\n",
    "        - class_weight: Mapping | str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizando o HistGradientBoostingClassifier com o GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "param_grid = {\n",
    "                'loss': ['log_loss'],\n",
    "                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                'max_iter': [50, 125, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'l2_regularization': [0.1, 0.01, 0.001]}\n",
    "grid_search = GridSearchCV(HistGradientBoostingClassifier(random_state=0), param_grid=param_grid)\n",
    "grid_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l2_regularization': 0.1, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_iter': 125}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       150\n",
      "   macro avg       1.00      1.00      1.00       150\n",
      "weighted avg       1.00      1.00      1.00       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X)\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "    Para Reduzir Overfitting do GBM foi desenvolvida uma maneira de diminuir a correlação entre as árvores através de uma sub-amostragem\n",
    "    aleatoria dos dados de treino de cada árvore. Portanto no SGBM a principal diferença é que no GBM todas as árvores são treinadas com \n",
    "    todo o conjunto de dados enquanto no SGBM é feita essa sub-amostragem aleatória dos dados para todas as árvores. isso também é um hipermarametro do SGBM chamado de subsample. Você pode aumentar por exemplo de 0.5 metade dos dados para cada árvore até 1 que basicamente é o GBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
